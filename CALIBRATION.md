# Eye Tracking Calibration Enhancement Plan

## Document Overview

This document outlines the comprehensive phased implementation plan for enhancing the eye tracking calibration system. The goal is to improve calibration accuracy, user experience, and provide robust model management capabilities.

**Created:** 2025-11-11
**Status:** In Progress
**Current Phase:** Phase 1 - Model Selection Infrastructure

---

## Executive Summary

### Current Implementation Status

**Existing Calibration Features:**
- ✅ Basic 5-point calibration (4 corners + center)
- ✅ 3-second dwell time per calibration point
- ✅ Fullscreen calibration mode
- ✅ Basic progress indicator
- ✅ Simple cancel functionality

**Existing Model Infrastructure:**
- ✅ Robust ModelRegistry with database persistence
- ✅ Support for YOLO (nano, small, medium, large, xlarge)
- ✅ Support for YuNet, Haar Cascade, MediaPipe
- ✅ Platform-specific model selection
- ✅ Performance ratings (accuracy, speed)
- ✅ Model download tracking

**Missing Features:**
- ❌ Visual/audio countdown before calibration points
- ❌ Text-to-speech guidance
- ❌ Enhanced data capture (landmarks, detailed pose, gaze vectors)
- ❌ Non-intrusive real-time data display
- ❌ Instructions overlay during calibration
- ❌ Camera and model selection within calibration UI
- ❌ Customizable circle duration
- ❌ Use of calibration data to improve tracking accuracy
- ❌ Admin UI for model management

---

## Implementation Phases

### Phase 1: Model Selection Infrastructure ✅

**Status:** Completed
**Completion Date:** 2025-11-11
**Actual Duration:** 2 hours
**Complexity:** Medium

#### Goals
Enable users to select AI models (YOLO variants, YuNet, MediaPipe, Haar Cascade) during calibration and testing, with model information display and persistence.

#### Deliverables
1. `ModelSelectionDialog` widget with model cards showing:
   - Model name, type, and variant (nano, medium, large, etc.)
   - Accuracy and speed ratings (visual indicators)
   - Platform compatibility
   - Download status (bundled, downloaded, available)
   - File size and description
2. Integration with existing `ModelRegistry` service
3. Model selection UI in `CalibrationPage`
4. Wire Flutter model selection to native C++ layer
5. Model selection persistence across sessions

#### Files to Modify/Create
- **NEW:** `flutter_app/lib/widgets/model_selection_dialog.dart` (220+ lines)
- **MODIFY:** `flutter_app/lib/pages/calibration_page.dart` (add model selector button/dropdown)
- **MODIFY:** `flutter_app/lib/services/camera_service.dart` (add `setModel(modelId)` method)
- **MODIFY:** `flutter_app/macos/Runner/EyeTrackingPlugin.swift` (add `setModel` method channel handler)
- **MODIFY:** `core/include/tracking_engine.h` (add `setModel(path, type, variant)` method)
- **MODIFY:** `core/src/tracking_engine.cpp` (implement model loading and switching)

#### Implementation Steps
1. Create `ModelSelectionDialog` widget:
   - Fetch available models from `ModelRegistry`
   - Display models in card grid layout
   - Show model metadata (accuracy, speed, size)
   - Handle model selection and return selected model ID
   - Show download prompt for unavailable models

2. Modify `CameraService`:
   - Add `Future<void> setModel(String modelId)` method
   - Call native platform channel to load model
   - Handle model loading errors
   - Store selected model ID in preferences

3. Update native layer:
   - Add method channel handler for `setModel`
   - Implement model file path resolution
   - Load model into tracking engine
   - Return success/failure status

4. Integrate into `CalibrationPage`:
   - Add "Select Model" button in pre-calibration screen
   - Show currently selected model
   - Update UI when model changes

#### Test Scenarios
1. **Model Selection Flow**
   - Launch calibration page
   - Tap "Select Model" button
   - View list of available models
   - Select different YOLO variant
   - Verify model selection persists

2. **Model Performance**
   - Select YOLO nano (fast, less accurate)
   - Start calibration
   - Verify detection works and is fast
   - Switch to YOLO xlarge (slow, more accurate)
   - Verify detection is more accurate but slower

3. **Model Unavailability**
   - Select model marked "Download Required"
   - Verify download prompt appears
   - Cancel download
   - Verify default model still used

4. **Cross-Platform**
   - Test on macOS with CoreML models
   - Test on Windows with ONNX models
   - Verify platform-appropriate models shown

5. **Persistence**
   - Select YOLO medium
   - Close calibration page
   - Reopen calibration page
   - Verify YOLO medium still selected

#### Test Cases

**TC1.1: Select YOLO Nano Model**
- Precondition: Calibration page open
- Steps: Tap "Select Model" → Select "YOLO11 Nano" → Tap "Confirm"
- Expected: Model selected, detection runs at high FPS (>25), face detected quickly
- Actual:
- Status:

**TC1.2: Select YOLO XLarge Model**
- Precondition: Calibration page open
- Steps: Tap "Select Model" → Select "YOLO11 XLarge" → Tap "Confirm"
- Expected: Model selected, detection more accurate, FPS may be lower (15-20)
- Actual:
- Status:

**TC1.3: Unavailable Model Download Prompt**
- Precondition: Model not downloaded
- Steps: Tap "Select Model" → Select unavailable model → Observe prompt
- Expected: Dialog appears: "Model not downloaded. Download now?"
- Actual:
- Status:

**TC1.4: Cancel Model Selection**
- Precondition: Current model is YOLO Medium
- Steps: Tap "Select Model" → Select YOLO Large → Tap "Cancel"
- Expected: Dialog closes, YOLO Medium still active
- Actual:
- Status:

**TC1.5: Model Selection Persistence**
- Precondition: None
- Steps: Select YOLO Large → Close app → Reopen app → Open calibration
- Expected: YOLO Large still selected
- Actual:
- Status:

#### Success Criteria
- [ ] Model selection dialog displays all available models
- [ ] Model selection persists across app restarts
- [ ] Selected model is successfully loaded in native layer
- [ ] Face detection works with selected model
- [ ] All 5 test cases pass

#### Dependencies
- None (foundational phase)

#### Risks & Mitigation
- **Risk:** Model loading fails on native layer
  - **Mitigation:** Implement fallback to default model, show error to user
- **Risk:** Large models cause performance issues
  - **Mitigation:** Show performance warning for xlarge models, recommend medium

---

### Phase 2: Enhanced Calibration Data Capture

**Status:** Not Started
**Estimated Duration:** 2-3 weeks
**Complexity:** High

#### Goals
Capture comprehensive tracking data for each calibration point including face landmarks, eye landmarks, head pose (pitch/yaw/roll), gaze vectors, and shoulder position.

#### Deliverables
1. Extended `TrackingResult` model with:
   - Face landmarks (68-point or 478-point MediaPipe)
   - Eye landmarks (left and right eye contours)
   - Head pose (pitch, yaw, roll in degrees)
   - 3D gaze vector (direction of gaze)
   - Shoulder landmarks
   - Detection confidence score

2. `CalibrationDataPoint` model to store:
   - Target position (x, y)
   - Full tracking result at that point
   - Timestamp
   - Dwell time

3. `CalibrationSession` model to store:
   - Session ID and timestamp
   - User ID
   - Model used
   - Camera used
   - List of calibration data points
   - Quality metrics

4. Native layer modifications to extract landmark data
5. Data persistence (save/load calibration sessions)

#### Files to Modify/Create
- **MODIFY:** `flutter_app/lib/models/app_state.dart` (extend `TrackingResult`)
- **NEW:** `flutter_app/lib/models/calibration_data.dart` (300+ lines)
- **MODIFY:** `flutter_app/lib/services/camera_service.dart` (parse extended tracking data)
- **MODIFY:** `core/include/tracking_engine.h` (add landmarks to TrackingResult struct)
- **MODIFY:** `core/src/tracking_engine.cpp` (extract and populate landmark data)
- **MODIFY:** `flutter_app/macos/Runner/EyeTrackingPlugin.swift` (serialize extended data)

#### Data Structures

```dart
class ExtendedTrackingResult extends TrackingResult {
  final List<Point> faceLandmarks;      // 68 or 478 points
  final List<Point> leftEyeLandmarks;   // ~6 points
  final List<Point> rightEyeLandmarks;  // ~6 points
  final Vector3 headPose;               // (pitch, yaw, roll) in degrees
  final Vector3 gazeVector;             // (x, y, z) normalized direction
  final List<Point>? shoulderLandmarks; // 2 points (optional)
  final double confidence;              // 0.0 to 1.0
}

class CalibrationDataPoint {
  final Offset targetPosition;          // Where circle was displayed
  final ExtendedTrackingResult tracking; // What was detected
  final DateTime timestamp;
  final Duration dwellTime;              // How long user looked
}

class CalibrationSession {
  final String id;
  final String userId;
  final DateTime createdAt;
  final String modelId;
  final String cameraId;
  final List<CalibrationDataPoint> dataPoints;
  final double? qualityScore;            // Calculated post-session
  final Map<String, dynamic> metadata;
}
```

#### Implementation Steps
1. Extend data models in Flutter
2. Modify C++ tracking engine to extract landmarks using OpenCV/MediaPipe
3. Update platform channel serialization to pass landmark arrays
4. Modify `CameraService` to parse extended data
5. Store calibration sessions in SQLite
6. Implement calibration session save/load

#### Test Scenarios
1. Run calibration, verify 5 data points captured with full tracking data
2. Export calibration data to JSON, verify all fields populated
3. Test with face turned away, verify confidence scores reflect poor quality
4. Test with eyes closed, verify eye landmarks missing or flagged
5. Save and reload calibration session, verify no data loss

#### Test Cases

**TC2.1: Face Landmarks Captured**
- Precondition: Calibration running
- Steps: Complete calibration point 1 → Check captured data
- Expected: faceLandmarks.length == 68 (or 478 for MediaPipe)
- Actual:
- Status:

**TC2.2: Head Pose in Valid Range**
- Precondition: User looking straight at screen
- Steps: Complete calibration → Check headPose values
- Expected: pitch ≈ 0±15°, yaw ≈ 0±15°, roll ≈ 0±15°
- Actual:
- Status:

**TC2.3: Gaze Vector Accuracy**
- Precondition: Calibration point in top-right corner
- Steps: User looks at target → Capture → Check gazeVector
- Expected: gazeVector points approximately toward top-right (x>0, y<0)
- Actual:
- Status:

**TC2.4: Shoulder Detection**
- Precondition: User seated, shoulders visible
- Steps: Complete all 5 calibration points → Check shoulder data
- Expected: shoulderLandmarks detected in at least 3 of 5 points
- Actual:
- Status:

**TC2.5: Session Persistence**
- Precondition: Calibration completed
- Steps: Save session → Close app → Reopen → Load session
- Expected: All calibration data intact, no data loss
- Actual:
- Status:

#### Success Criteria
- [ ] All landmarks extracted and stored correctly
- [ ] Head pose angles calculated accurately
- [ ] Gaze vectors point in expected directions
- [ ] Calibration sessions persist to database
- [ ] All 5 test cases pass

#### Dependencies
- Phase 1 (model selection affects what data is available)

#### Risks & Mitigation
- **Risk:** C++ landmark extraction complex, may require MediaPipe integration
  - **Mitigation:** Start with OpenCV's simpler face landmarks, upgrade to MediaPipe later
- **Risk:** Large data size (478 points × 5 calibration points)
  - **Mitigation:** Compress data, store only essential points, use binary format

---

### Phase 3: UI/UX Improvements - Countdown, Instructions, Data Display

**Status:** Not Started
**Estimated Duration:** 1-2 weeks
**Complexity:** Medium

#### Goals
Create non-intrusive visual feedback and guidance during calibration including countdown timer, flash effect, instructions overlay, and real-time data display.

#### Deliverables
1. `CountdownOverlay` widget:
   - Circular countdown animation (5, 4, 3, 2, 1)
   - Large center number
   - White flash effect on completion
   - Configurable duration

2. `InstructionsOverlay` widget:
   - Semi-transparent panel (bottom or top of screen)
   - Current instruction text
   - Progress indicator (1/5, 2/5, etc.)
   - Tips ("Keep your head still", "Blink normally")
   - Toggleable on/off

3. `CalibrationDataOverlay` widget:
   - Small corner panel showing real-time data
   - Face distance, head pose, gaze angle, confidence
   - Color-coded indicators (green/yellow/red)
   - Toggleable on/off

4. Customizable circle duration in calibration settings

#### Files to Modify/Create
- **NEW:** `flutter_app/lib/widgets/countdown_overlay.dart` (180+ lines)
- **NEW:** `flutter_app/lib/widgets/instructions_overlay.dart` (150+ lines)
- **NEW:** `flutter_app/lib/widgets/calibration_data_overlay.dart` (200+ lines)
- **MODIFY:** `flutter_app/lib/pages/calibration_page.dart` (integrate all overlays)
- **NEW:** `flutter_app/lib/widgets/calibration_settings_dialog.dart` (120+ lines)

#### UI Design Specifications

**Countdown Overlay:**
- Position: Center of screen or near target circle
- Size: 150×150 px circular indicator
- Animation: Arc sweeps from 0° to 360° over countdown duration
- Number: 72pt font, white, center-aligned
- Flash: Full-screen white overlay, opacity 0.8, fade out over 300ms
- Timing: Appears 5 seconds before each circle, counts down, then flash

**Instructions Overlay:**
- Position: Bottom 15% of screen, centered horizontally
- Background: Semi-transparent black (opacity 0.7)
- Text: White, 18pt, center-aligned
- Examples:
  - "Look at the yellow circle in the top-left corner"
  - "Keep your head still and follow the circle with your eyes"
  - "Point 3 of 5 - Almost there!"
- Can be dismissed or hidden via settings

**Data Display Overlay:**
- Position: Top-right corner (or user-configurable)
- Size: 200×150 px
- Background: Semi-transparent dark (opacity 0.6)
- Content:
  - Face Distance: 45cm ● (green if 40-60cm, yellow if 30-70cm, red otherwise)
  - Head Pose: Pitch 5° Yaw -2° Roll 1° ●●●
  - Gaze: X: 15° Y: -8° ●●
  - Confidence: 94% ●●●●●
- Updates: 10+ times per second
- Can be hidden via settings

#### Implementation Steps
1. Create countdown overlay with animation
2. Create instructions overlay with dynamic text
3. Create data display overlay with real-time updates
4. Add settings dialog for customization
5. Integrate all overlays into calibration page
6. Add show/hide toggles

#### Test Scenarios
1. Start calibration, verify 5-second countdown appears before first point
2. Verify white flash occurs when countdown reaches 0
3. Verify instructions update for each calibration point
4. Verify data display shows real-time tracking data without blocking circles
5. Change circle duration from 3s to 5s, verify countdown duration changes
6. Hide data display in settings, verify overlay hidden during calibration

#### Test Cases

**TC3.1: Countdown Animation**
- Precondition: Calibration started
- Steps: Observe countdown before first circle
- Expected: Shows 5, 4, 3, 2, 1 with circular arc animation
- Actual:
- Status:

**TC3.2: Flash Effect**
- Precondition: Countdown reaches 0
- Steps: Observe flash effect
- Expected: Full-screen white flash, ~300ms duration, fades out smoothly
- Actual:
- Status:

**TC3.3: Instructions Non-Blocking**
- Precondition: Calibration point in bottom-center
- Steps: Read instructions while looking at circle
- Expected: Instructions visible but don't obscure calibration circle
- Actual:
- Status:

**TC3.4: Real-Time Data Updates**
- Precondition: Calibration running, data overlay enabled
- Steps: Move head slightly → Observe data overlay
- Expected: Head pose values update at least 10 times/second
- Actual:
- Status:

**TC3.5: Corner Circle Not Blocked**
- Precondition: Data overlay enabled, top-right position
- Steps: Calibration point appears in top-right corner
- Expected: Data overlay repositions or becomes transparent to not block circle
- Actual:
- Status:

**TC3.6: Customizable Duration**
- Precondition: Settings accessible
- Steps: Change circle duration from 3s to 5s → Start calibration
- Expected: Each circle stays for 5 seconds instead of 3
- Actual:
- Status:

#### Success Criteria
- [ ] Countdown animation smooth and visible
- [ ] Flash effect noticeable but not jarring
- [ ] Instructions clear and helpful
- [ ] Data overlay updates in real-time
- [ ] No overlays block calibration circles
- [ ] All 6 test cases pass

#### Dependencies
- Phase 2 (for real-time data to display in overlay)

#### Risks & Mitigation
- **Risk:** Overlays cause performance issues or frame drops
  - **Mitigation:** Optimize rendering, use cached painters, test on low-end devices
- **Risk:** Flash effect too bright or causes accessibility issues
  - **Mitigation:** Make flash intensity configurable, add accessibility warnings

---

### Phase 4: Text-to-Speech Integration

**Status:** Not Started
**Estimated Duration:** 1 week
**Complexity:** Low-Medium

#### Goals
Integrate text-to-speech for countdown and instructions to improve accessibility and user guidance.

#### Deliverables
1. `TTSService` wrapper around `flutter_tts` plugin
2. Countdown speech ("Five", "Four", "Three", "Two", "One", "Begin")
3. Instruction speech ("Look at the circle in the top-left corner")
4. Completion speech ("Calibration complete. Thank you.")
5. Error speech ("Face not detected. Please adjust your position.")
6. TTS settings (enable/disable, voice, speed, volume, pitch)

#### Files to Modify/Create
- **NEW:** `flutter_app/lib/services/tts_service.dart` (250+ lines)
- **MODIFY:** `flutter_app/pubspec.yaml` (add `flutter_tts: ^4.0.2` dependency)
- **MODIFY:** `flutter_app/lib/pages/calibration_page.dart` (integrate TTS calls)
- **MODIFY:** `flutter_app/lib/models/app_state.dart` (add TTS settings to AppSettings)
- **NEW:** `flutter_app/lib/widgets/tts_settings_dialog.dart` (180+ lines)

#### TTS Integration Details

**Speech Events:**
1. **Calibration Start:** "Starting calibration. Please follow the yellow circle with your eyes."
2. **Countdown:** "Five... Four... Three... Two... One... Begin."
3. **Point Instructions:** "Look at the circle in the [top-left/top-right/center/bottom-left/bottom-right] corner."
4. **Completion:** "Calibration complete. Thank you."
5. **Errors:** "Face not detected. Please adjust your position and try again."

**Settings:**
- Enable/Disable TTS
- Voice selection (system voices)
- Speech rate (0.5x to 2.0x)
- Volume (0% to 100%)
- Pitch (0.5 to 2.0)

#### Implementation Steps
1. Add `flutter_tts` dependency to pubspec.yaml
2. Create `TTSService` with initialization, speak, stop methods
3. Add TTS settings to `AppSettings` model
4. Create TTS settings dialog
5. Integrate TTS calls into calibration page at appropriate moments
6. Test on macOS, Windows, iOS, Android

#### Test Scenarios
1. Enable TTS, start calibration, verify countdown spoken aloud
2. Verify instructions spoken at each calibration point
3. Adjust voice speed to 0.5x, verify slow speech
4. Disable TTS, verify no speech during calibration
5. Test on multiple platforms (macOS, Windows, mobile)

#### Test Cases

**TC4.1: Countdown Speech**
- Precondition: TTS enabled, calibration started
- Steps: Listen during countdown
- Expected: Hears "Five, Four, Three, Two, One, Begin" in sync with visual countdown
- Actual:
- Status:

**TC4.2: Instruction Speech**
- Precondition: TTS enabled
- Steps: Complete first point → Listen for instruction
- Expected: Hears "Look at the circle in the [position]"
- Actual:
- Status:

**TC4.3: Speech Rate Adjustment**
- Precondition: TTS enabled
- Steps: Set speech rate to 0.5x → Start calibration
- Expected: Speech noticeably slower than normal
- Actual:
- Status:

**TC4.4: TTS Disabled**
- Precondition: TTS disabled in settings
- Steps: Start calibration
- Expected: No audio output during calibration
- Actual:
- Status:

**TC4.5: Cross-Platform Compatibility**
- Precondition: TTS enabled
- Steps: Test on macOS, Windows, iOS, Android
- Expected: TTS works on all platforms without errors
- Actual:
- Status:

#### Success Criteria
- [ ] TTS speaks countdown in sync with visual countdown
- [ ] Instructions spoken clearly and at appropriate times
- [ ] TTS settings work correctly (voice, speed, volume)
- [ ] Can disable TTS completely
- [ ] Works on macOS, Windows, iOS, Android
- [ ] All 5 test cases pass

#### Dependencies
- None (independent feature)

#### Risks & Mitigation
- **Risk:** TTS voice quality varies by platform
  - **Mitigation:** Test on all platforms, document voice quality, allow users to disable
- **Risk:** TTS latency causes speech to lag behind visual countdown
  - **Mitigation:** Pre-cache speech, start TTS slightly before visual countdown

---

### Phase 5: Camera & Model Selection in Calibration Window

**Status:** Not Started
**Estimated Duration:** 1 week
**Complexity:** Low-Medium

#### Goals
Enable camera and model selection directly within the calibration UI, with live preview before calibration starts.

#### Deliverables
1. Pre-calibration setup screen with:
   - Live camera preview showing face detection
   - Camera selector dropdown
   - Model selector dropdown
   - Settings panel (circle duration, countdown, TTS toggles)
   - "Start Calibration" button

2. Camera preview with real-time detection overlay
3. Model and camera selection persistence
4. Display current camera and model info

#### Files to Modify/Create
- **MODIFY:** `flutter_app/lib/pages/calibration_page.dart` (add pre-calibration screen)
- **MODIFY:** `flutter_app/lib/widgets/camera_selection_dialog.dart` (reuse/enhance existing)
- **USE:** `flutter_app/lib/widgets/model_selection_dialog.dart` (from Phase 1)
- **NEW:** `flutter_app/lib/widgets/calibration_preview.dart` (camera preview widget)
- **NEW:** `flutter_app/lib/widgets/calibration_setup_panel.dart` (settings panel)

#### UI Design

**Pre-Calibration Screen Layout:**
```
┌─────────────────────────────────────────────┐
│ Camera Preview (640×480)                    │
│ [Live video with face detection box]        │
│                                             │
└─────────────────────────────────────────────┘
┌─────────────────────────────────────────────┐
│ Camera: [Built-in Camera ▼]                │
│ Model:  [YOLO11 Medium (Balanced) ▼]       │
│                                             │
│ Settings:                                   │
│ ☑ Countdown (5 seconds)                    │
│ ☑ Text-to-Speech                           │
│ Circle Duration: [3 seconds ▼]             │
│                                             │
│           [Start Calibration]               │
└─────────────────────────────────────────────┘
```

#### Implementation Steps
1. Create calibration preview widget with camera feed
2. Add camera and model dropdowns to calibration page
3. Create settings panel for calibration options
4. Implement pre-calibration screen state management
5. Wire "Start Calibration" button to begin calibration process
6. Save selections to preferences

#### Test Scenarios
1. Open calibration, switch camera, verify preview updates
2. Select different model, verify face detection changes in preview
3. Start calibration with selected camera/model, verify they are used
4. Close and reopen calibration, verify last-used camera/model selected
5. Change settings (duration, countdown), verify applied during calibration

#### Test Cases

**TC5.1: Camera Switching**
- Precondition: Multiple cameras available
- Steps: Open calibration → Select external webcam from dropdown
- Expected: Preview switches to show external webcam feed
- Actual:
- Status:

**TC5.2: Model Preview**
- Precondition: Pre-calibration screen open
- Steps: Select YOLO nano → Observe detection in preview
- Expected: Face detection box appears, updates quickly (>25 FPS)
- Actual:
- Status:

**TC5.3: Settings Applied**
- Precondition: Pre-calibration screen
- Steps: Set duration to 5s, disable TTS → Start calibration
- Expected: Circles stay 5s each, no speech during calibration
- Actual:
- Status:

**TC5.4: Selection Persistence**
- Precondition: None
- Steps: Select Camera 2, YOLO Large → Close app → Reopen calibration
- Expected: Camera 2 and YOLO Large pre-selected
- Actual:
- Status:

**TC5.5: Default Selection**
- Precondition: First-time user, no saved preferences
- Steps: Open calibration
- Expected: Default camera and default model (YOLO Medium) selected
- Actual:
- Status:

#### Success Criteria
- [ ] Camera preview shows live detection
- [ ] Camera and model switching works smoothly
- [ ] Selections persist across sessions
- [ ] Settings applied correctly during calibration
- [ ] All 5 test cases pass

#### Dependencies
- Phase 1 (model selection dialog)

#### Risks & Mitigation
- **Risk:** Multiple camera switching causes delays
  - **Mitigation:** Initialize cameras lazily, show loading indicator
- **Risk:** Preview consumes too much CPU/battery
  - **Mitigation:** Reduce preview frame rate (15 FPS), pause when not visible

---

### Phase 6: Post-Calibration Tracking Improvements

**Status:** Not Started
**Estimated Duration:** 2-3 weeks
**Complexity:** High

#### Goals
Use captured calibration data to build user-specific profiles that improve real-time tracking accuracy through gaze offset correction, head pose normalization, and eye-to-screen mapping.

#### Deliverables
1. `CalibrationProfile` model with correction matrices
2. `CalibrationService` for profile management
3. Gaze offset correction algorithm
4. Head pose baseline normalization
5. Eye-to-screen coordinate mapping (polynomial or homography)
6. Calibration quality score calculation
7. Profile save/load functionality
8. Recalibration prompt when quality degrades

#### Files to Modify/Create
- **NEW:** `flutter_app/lib/services/calibration_service.dart` (400+ lines)
- **NEW:** `flutter_app/lib/models/calibration_profile.dart` (250+ lines)
- **MODIFY:** `flutter_app/lib/services/camera_service.dart` (apply calibration corrections)
- **MODIFY:** `core/include/tracking_engine.h` (add calibration methods)
- **MODIFY:** `core/src/tracking_engine.cpp` (implement correction algorithms)
- **NEW:** `flutter_app/lib/widgets/calibration_quality_report.dart` (200+ lines)

#### Calibration Algorithms

**1. Gaze Offset Correction:**
- Calculate gaze error at each calibration point
- Build 2D polynomial mapping: `correctedGaze = f(rawGaze, headPose)`
- Apply correction in real-time tracking

**2. Head Pose Baseline:**
- Store user's neutral head position (pitch, yaw, roll)
- Normalize all head pose measurements relative to baseline
- Accounts for users who naturally tilt their head

**3. Eye-to-Screen Mapping:**
- Use calibration points to build homography matrix
- Map eye gaze vector to screen coordinates
- Handle perspective distortion

**4. Confidence Weighting:**
- Weight calibration points by detection confidence
- Discard outliers (>2 standard deviations from mean)
- Require minimum 3 high-confidence points for valid calibration

**5. Quality Score Calculation:**
```
qualityScore = (
  0.4 * confidenceAverage +
  0.3 * gazeAccuracyScore +
  0.2 * headPoseConsistency +
  0.1 * completenessScore
) * 100
```

#### Data Structures

```dart
class CalibrationProfile {
  final String id;
  final String userId;
  final DateTime createdAt;
  final List<CalibrationDataPoint> dataPoints;

  // Correction parameters
  final List<double> gazeOffsetMatrix;    // 2D polynomial coefficients
  final Vector3 headPoseBaseline;         // Neutral head position
  final List<double> homographyMatrix;    // 3×3 eye-to-screen mapping

  // Quality metrics
  final double qualityScore;              // 0-100
  final double averageConfidence;
  final double gazeAccuracy;              // Degrees of error
  final double headPoseConsistency;       // Variance in head pose

  // Metadata
  final String modelId;
  final String cameraId;
  final Map<String, dynamic> metadata;
}
```

#### Implementation Steps
1. Create `CalibrationProfile` model with correction parameters
2. Implement quality score calculation algorithm
3. Implement gaze offset correction algorithm
4. Implement head pose baseline normalization
5. Create `CalibrationService` to manage profiles
6. Modify tracking engine to apply corrections
7. Create quality report UI
8. Implement recalibration prompt logic

#### Test Scenarios
1. Complete good calibration, verify quality score >80
2. Run tracking test, measure gaze accuracy improvement
3. Save profile, reload, verify corrections still applied
4. Do intentionally poor calibration, verify low quality score (<50)
5. Test recalibration prompt after 30 minutes of use

#### Test Cases

**TC6.1: High Quality Calibration**
- Precondition: User follows instructions carefully
- Steps: Complete calibration → Check quality score
- Expected: Quality score > 80
- Actual:
- Status:

**TC6.2: Accuracy Improvement**
- Precondition: Calibration profile created
- Steps: Run gaze accuracy test → Compare with/without calibration
- Expected: Accuracy improves by at least 20% (e.g., 5° error → 4° error)
- Actual:
- Status:

**TC6.3: Head Pose Normalization**
- Precondition: User naturally tilts head 10° to the left
- Steps: Complete calibration → Track head pose during test
- Expected: Head pose values normalized, tilt accounted for
- Actual:
- Status:

**TC6.4: Profile Persistence**
- Precondition: Calibration completed, profile saved
- Steps: Close app → Reopen → Run tracking test
- Expected: Same accuracy as before app closed
- Actual:
- Status:

**TC6.5: Low Quality Warning**
- Precondition: Calibration with poor data (moving head, looking away)
- Steps: Complete calibration → Check quality score
- Expected: Quality score < 50, warning shown: "Low calibration quality. Please recalibrate."
- Actual:
- Status:

#### Success Criteria
- [ ] Calibration quality score accurately reflects calibration quality
- [ ] Gaze accuracy improves by at least 20% with calibration
- [ ] Head pose baseline correctly accounts for user's natural posture
- [ ] Profiles save and load correctly
- [ ] All 5 test cases pass

#### Dependencies
- Phase 2 (requires calibration data points)

#### Risks & Mitigation
- **Risk:** Calibration algorithms complex, may require mathematical expertise
  - **Mitigation:** Start with simple linear correction, iterate with polynomial if needed
- **Risk:** Correction algorithms introduce errors or artifacts
  - **Mitigation:** Extensive testing, provide option to disable corrections
- **Risk:** Profile becomes invalid over time (lighting changes, glasses on/off)
  - **Mitigation:** Implement recalibration prompts, profile expiration

---

### Phase 7: Admin Model Management UI

**Status:** Not Started
**Estimated Duration:** 2 weeks
**Complexity:** Medium-High

#### Goals
Provide administrators with comprehensive model management capabilities including adding custom models, downloading models, testing models, and configuring default models.

#### Deliverables
1. Admin model management page accessible from admin dashboard
2. Model list with detailed information and status
3. Model upload functionality for custom models
4. Model download functionality with progress tracking
5. Model testing with live camera preview
6. Set default model per platform
7. Delete custom models (bundled models protected)
8. Model metadata editor

#### Files to Modify/Create
- **NEW:** `flutter_app/lib/pages/admin_model_management.dart` (500+ lines)
- **NEW:** `flutter_app/lib/widgets/model_card.dart` (250+ lines)
- **NEW:** `flutter_app/lib/widgets/model_upload_dialog.dart` (300+ lines)
- **NEW:** `flutter_app/lib/services/model_download_service.dart` (350+ lines)
- **MODIFY:** `flutter_app/lib/pages/admin_dashboard.dart` (add navigation link)
- **MODIFY:** `flutter_app/lib/services/model_registry.dart` (add download progress tracking)

#### UI Design

**Model Management Page:**
```
┌──────────────────────────────────────────────────┐
│ Model Management                 [+ Add Model]   │
├──────────────────────────────────────────────────┤
│                                                  │
│ ┌────────────────────────────────────────────┐  │
│ │ YOLO11 Medium (CoreML)            ★ Default│  │
│ │ Size: 52MB  •  Accuracy: ●●●●○  Speed: ●●●○│  │
│ │ Status: ✓ Bundled  •  Platforms: macOS     │  │
│ │ [Test] [Set Default] [Details]             │  │
│ └────────────────────────────────────────────┘  │
│                                                  │
│ ┌────────────────────────────────────────────┐  │
│ │ YOLO11 XLarge (CoreML)                     │  │
│ │ Size: 128MB  •  Accuracy: ●●●●●  Speed: ●●○│  │
│ │ Status: ⬇ Not Downloaded                   │  │
│ │ [Download] [Details]                       │  │
│ └────────────────────────────────────────────┘  │
│                                                  │
│ ┌────────────────────────────────────────────┐  │
│ │ Custom YOLOv8 (ONNX)                       │  │
│ │ Size: 45MB  •  Accuracy: ●●●○○  Speed: ●●●●│  │
│ │ Status: ✓ Downloaded  •  Platforms: All   │  │
│ │ [Test] [Delete] [Edit] [Details]          │  │
│ └────────────────────────────────────────────┘  │
│                                                  │
└──────────────────────────────────────────────────┘
```

**Model Upload Dialog:**
- File picker for model file (.mlmodel, .onnx, .tflite, .xml)
- Metadata form:
  - Model name
  - Model type (YOLO, YuNet, MediaPipe, Haar, Custom)
  - Variant (nano, small, medium, large, xlarge, standard)
  - Platform compatibility (checkboxes)
  - Accuracy rating (slider 0.0-1.0)
  - Speed rating (slider 0.0-1.0)
  - Description
- Upload button

#### Implementation Steps
1. Create admin model management page
2. Create model card widget to display model info
3. Implement model list with sorting and filtering
4. Create model upload dialog and file handling
5. Implement model download service with progress tracking
6. Create model testing feature with camera preview
7. Implement set default model functionality
8. Implement delete model with confirmation
9. Add navigation from admin dashboard

#### Test Scenarios
1. Admin navigates to model management page
2. Admin downloads YOLO xlarge model, verify progress shown, file saved
3. Admin uploads custom model file, fills metadata, verify appears in list
4. Admin tests model with live camera, verify detection works
5. Admin deletes custom model, verify file removed and registry updated
6. Admin sets YOLO large as default, user sees it as default in calibration
7. Admin attempts to delete bundled model, verify error message

#### Test Cases

**TC7.1: Model Download**
- Precondition: Admin logged in, model not downloaded
- Steps: Navigate to model management → Click "Download" on YOLO XLarge
- Expected: Progress bar shows, model downloads, status changes to "Downloaded"
- Actual:
- Status:

**TC7.2: Custom Model Upload**
- Precondition: Admin has custom ONNX model file
- Steps: Click "+ Add Model" → Select file → Fill metadata → Upload
- Expected: Model appears in list, usable in calibration
- Actual:
- Status:

**TC7.3: Delete Bundled Model**
- Precondition: Admin viewing bundled YOLO Medium
- Steps: Click "Delete" on bundled model
- Expected: Error dialog: "Cannot delete bundled models"
- Actual:
- Status:

**TC7.4: Test Model**
- Precondition: Model downloaded
- Steps: Click "Test" → Allow camera access → Observe preview
- Expected: Camera preview opens, face detection box appears
- Actual:
- Status:

**TC7.5: Set Default Model**
- Precondition: Admin logged in
- Steps: Click "Set Default" on YOLO Large → Logout → Login as user
- Expected: User sees YOLO Large as default in calibration
- Actual:
- Status:

#### Success Criteria
- [ ] Admin can view all models with status and metadata
- [ ] Model download works with progress tracking
- [ ] Custom model upload works and models are usable
- [ ] Model testing shows live detection
- [ ] Cannot delete bundled models
- [ ] Set default model works for all users
- [ ] All 5 test cases pass

#### Dependencies
- Phase 1 (model selection infrastructure)

#### Risks & Mitigation
- **Risk:** Large model downloads fail or timeout
  - **Mitigation:** Implement resumable downloads, retry logic, checksum verification
- **Risk:** Custom models malformed or incompatible
  - **Mitigation:** Validate model files before accepting, test load before saving
- **Risk:** Security risk from uploading arbitrary files
  - **Mitigation:** Restrict file types, scan for malware, isolate model storage

---

## Testing & Documentation Process

### Per-Phase Workflow

After completing each phase:

1. **Development:**
   - Implement all deliverables
   - Write code following project style guide
   - Add code comments and documentation

2. **Testing:**
   - Write test scenarios and test cases
   - Perform manual testing
   - Record actual results in test case table
   - Mark pass/fail status
   - Fix any failed tests

3. **Build:**
   - Run `flutter clean` and `flutter pub get`
   - Build for target platforms (macOS, iOS, Windows, Android)
   - Verify no build errors
   - Test on physical devices if possible

4. **Documentation:**
   - Create `CALIBRATION-PhaseX.md` with:
     - Phase summary
     - Implementation notes
     - Test results (all test cases with actual results)
     - Build logs (summary)
     - Issues encountered and resolutions
     - Code changes summary
     - Screenshots/videos if applicable
     - Completion date
   - Update this `CALIBRATION.md`:
     - Mark phase status as "Completed"
     - Add completion date
     - Note any deviations from plan
     - Update dependencies if discovered

5. **Version Control:**
   - Stage all changes: `git add .`
   - Commit with descriptive message: `git commit -m "feat(calibration): Phase X - [Phase Name]"`
   - Push to remote: `git push origin main`

### Test Environment

**Platforms:**
- macOS 13+ (primary development)
- iOS 15+ (mobile testing)
- Windows 10+ (cross-platform verification)
- Android 10+ (mobile testing)

**Hardware:**
- Built-in camera (MacBook, iPhone)
- External USB webcam
- Different lighting conditions (bright, dim, natural)

**Test Users:**
- With/without glasses
- Different face shapes and skin tones
- Different seating distances (30cm - 80cm)

### Success Metrics

**Overall Project Success:**
- All 7 phases completed
- All test cases passing (35+ tests total)
- Calibration quality score average >75 across test users
- Gaze accuracy improves by >25% with calibration
- User can complete calibration in <2 minutes
- Admin can add/download/manage models without code changes
- App remains responsive (>20 FPS) during calibration
- No critical bugs or crashes

---

## Progress Tracking

### Phase Completion Status

| Phase | Status | Start Date | End Date | Test Pass Rate |
|-------|--------|------------|----------|----------------|
| Phase 1: Model Selection | ✅ Completed | 2025-11-11 | 2025-11-11 | 0 / 5 (manual testing required) |
| Phase 2: Data Capture | Not Started | - | - | - / 5 |
| Phase 3: UI/UX | Not Started | - | - | - / 6 |
| Phase 4: TTS | Not Started | - | - | - / 5 |
| Phase 5: Camera/Model in Cal | Not Started | - | - | - / 5 |
| Phase 6: Tracking Improvements | Not Started | - | - | - / 5 |
| Phase 7: Admin Model Mgmt | Not Started | - | - | - / 5 |

**Overall Progress:** 1 / 7 phases completed (14%)

### Recent Updates

**2025-11-11 (Phase 1 Completion):**
- ✅ Created ModelSelectionDialog widget with comprehensive UI
- ✅ Integrated model selection with CameraService
- ✅ Added native layer support in Swift plugin
- ✅ Updated calibration page with model selector button
- ✅ Implemented model persistence via SharedPreferences
- ✅ Build successful on macOS (arm64)
- ✅ Documentation: Created CALIBRATION-Phase1.md

**2025-11-11 (Planning):**
- ✅ Created comprehensive CALIBRATION.md plan
- ✅ Researched current codebase implementation
- ✅ Defined 7 phases with detailed deliverables
- ✅ Created 36 test cases across all phases

---

## Risk Management

### Identified Risks

1. **Technical Complexity (High):**
   - Calibration algorithms require mathematical expertise
   - Native C++ integration complex
   - **Mitigation:** Start simple, iterate, consult research papers

2. **Cross-Platform Compatibility (Medium):**
   - TTS quality varies by platform
   - Model formats differ (CoreML, ONNX, TFLite)
   - **Mitigation:** Extensive testing, fallback mechanisms

3. **Performance (Medium):**
   - Large models may cause frame drops
   - Multiple overlays may impact rendering
   - **Mitigation:** Optimize rendering, dynamic quality adjustment

4. **User Experience (Low):**
   - TTS may be annoying to some users
   - Too many overlays may be distracting
   - **Mitigation:** Make all features toggleable, user testing

5. **Data Storage (Low):**
   - Calibration data can be large (landmarks × 5 points)
   - **Mitigation:** Compress data, clean up old sessions

### Contingency Plans

- **If Phase 2 landmark extraction too complex:** Use OpenCV's simple landmarks instead of MediaPipe 478-point
- **If TTS not working on platform:** Make TTS optional, provide visual-only mode
- **If calibration algorithms don't improve accuracy:** Provide manual adjustment UI for users
- **If model downloads unreliable:** Provide bundled models only, make downloads optional

---

## References & Resources

### Technical Documentation
- OpenCV Face Landmark Detection: https://docs.opencv.org/4.x/d5/d47/tutorial_table_of_content_facemark.html
- MediaPipe Face Mesh: https://google.github.io/mediapipe/solutions/face_mesh.html
- Flutter TTS Plugin: https://pub.dev/packages/flutter_tts
- Eye Gaze Estimation: Research papers on polynomial regression mapping

### Project Files
- Model Registry: `flutter_app/lib/services/model_registry.dart`
- Current Calibration: `flutter_app/lib/pages/calibration_page.dart`
- Camera Service: `flutter_app/lib/services/camera_service.dart`
- Tracking Engine: `core/include/tracking_engine.h`

### Git Strategy
- Branch: `main` (direct commits per CLAUDE.md instructions)
- Commit format: `feat(calibration): Phase X - Description`
- Push after each phase completion

---

## Appendix

### Data Models Summary

```dart
// Extended tracking result with all landmarks
class ExtendedTrackingResult {
  List<Point> faceLandmarks;
  List<Point> leftEyeLandmarks;
  List<Point> rightEyeLandmarks;
  Vector3 headPose;      // (pitch, yaw, roll)
  Vector3 gazeVector;    // (x, y, z)
  List<Point>? shoulderLandmarks;
  double confidence;
}

// Single calibration data point
class CalibrationDataPoint {
  Offset targetPosition;
  ExtendedTrackingResult tracking;
  DateTime timestamp;
  Duration dwellTime;
}

// Complete calibration session
class CalibrationSession {
  String id;
  String userId;
  DateTime createdAt;
  String modelId;
  String cameraId;
  List<CalibrationDataPoint> dataPoints;
  double? qualityScore;
}

// User calibration profile for corrections
class CalibrationProfile {
  String id;
  String userId;
  List<double> gazeOffsetMatrix;
  Vector3 headPoseBaseline;
  List<double> homographyMatrix;
  double qualityScore;
  double averageConfidence;
  double gazeAccuracy;
  String modelId;
  String cameraId;
}
```

### File Structure

```
flutter_app/lib/
├── models/
│   ├── app_state.dart (MODIFY: extend TrackingResult)
│   ├── calibration_data.dart (NEW: CalibrationDataPoint, CalibrationSession)
│   └── calibration_profile.dart (NEW: CalibrationProfile)
├── services/
│   ├── camera_service.dart (MODIFY: setModel, parse extended data, apply corrections)
│   ├── model_registry.dart (MODIFY: download progress)
│   ├── calibration_service.dart (NEW: profile management)
│   ├── model_download_service.dart (NEW: download models)
│   └── tts_service.dart (NEW: text-to-speech)
├── pages/
│   ├── calibration_page.dart (MODIFY: add pre-screen, overlays, TTS)
│   ├── admin_dashboard.dart (MODIFY: add model management link)
│   └── admin_model_management.dart (NEW: model management UI)
└── widgets/
    ├── model_selection_dialog.dart (NEW: select models)
    ├── camera_selection_dialog.dart (MODIFY: enhance existing)
    ├── countdown_overlay.dart (NEW: countdown animation)
    ├── instructions_overlay.dart (NEW: instructions display)
    ├── calibration_data_overlay.dart (NEW: real-time data display)
    ├── calibration_settings_dialog.dart (NEW: calibration settings)
    ├── calibration_preview.dart (NEW: camera preview)
    ├── calibration_setup_panel.dart (NEW: setup panel)
    ├── calibration_quality_report.dart (NEW: quality metrics)
    ├── tts_settings_dialog.dart (NEW: TTS settings)
    ├── model_card.dart (NEW: model display card)
    └── model_upload_dialog.dart (NEW: upload custom models)

core/
├── include/
│   └── tracking_engine.h (MODIFY: add landmarks, setModel, calibration methods)
└── src/
    └── tracking_engine.cpp (MODIFY: extract landmarks, load models, apply corrections)

flutter_app/macos/Runner/
└── EyeTrackingPlugin.swift (MODIFY: setModel, serialize extended data)
```

---

**Document Version:** 1.0
**Last Updated:** 2025-11-11
**Next Review:** After Phase 1 completion
